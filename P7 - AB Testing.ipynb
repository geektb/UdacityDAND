{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7 - Design an A/B Test -\n",
    "##### Submitted by Ady Oren - 1/6/2016\n",
    "\n",
    "## [Experiment Overview: Free Trial Screener](https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True)\n",
    "\n",
    "At the time of this experiment, Udacity courses currently have two options on the home page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot shows](https://www.google.com/url?q=https://drive.google.com/a/knowlabs.com/file/d/0ByAfiG8HpNUMakVrS0s4cGN2TjQ/view?usp%3Dsharing&sa=D&ust=1483665587054000&usg=AFQjCNF2NgKQ6yp6gIScG_0PAdjCG45EbQ) what the experiment looks like.\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time—without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n",
    "\n",
    "### Metric Choice\n",
    "\n",
    "* **Number of cookies:** That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "* **Number of user-ids:** That is, number of users who enroll in the free trial. (dmin=50)\n",
    "* **Number of clicks:** That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "* **Click-through-probability:** That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "* **Gross conversion:** That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "* **Retention:** That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "* **Net conversion:** That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Choice\n",
    "##### List which metrics you will use as invariant metrics and evaluation metrics here.\n",
    "\n",
    "* **Invariant Metrics:** Number of cookies, Number of clicks, and Click through probability.\n",
    "* **Evaluation Metrics:** Gross conversion, Retention, and Net conversion.\n",
    "\n",
    "##### For each metric, explain both why you did or did not use it as an invariant metric and why you did or did not use it as an evaluation metric. Also, state what results you will look for in your evaluation metrics in order to launch the experiment.\n",
    "\n",
    "* **Number of cookies:** This is an invariant metric because the cookies are created on first visit to any page on the site and should not be affected by change.\n",
    "* **Number of user-ids:** Users IDs is not a good invariant because it can be affected by the change (number of new users who create an account in order to participate in free trial). It is also not a good evaluation metric because the mix of new vs existing users can be different between the control and experiment group.\n",
    "* **Number of clicks:** This is an invariant metric because the action of clicking on the \"start free trail\" happens before the 5-hour question is shown so the user experience should be the same across both groups.\n",
    "* **Click-through-probability:** Similar to the Number of Clicks, this is also a good invariant metric because the probability of click on the \"start free trail\" happens before any the user experience starts to differ between the groups.\n",
    "* **Gross conversion:** This is a good evaluation metric because it is dependent on the effect of the \"5 hours\" message on whether a user decides to sign up or not, and is part of what is being tested. it is not a good invariant metric because of its dependency.\n",
    "* **Retention:** This is also a good evaluation metric because it is dependent on the effect of the \"5 hours\" message since the point of the message was to decrease the number of users that cancel prior to the end of the 14-day trial. it is not a good invariant metric because of its dependency.\n",
    "* **Net conversion:** This is also a good evaluation metric because it includes the retention value which we expect to change based on the result of the change. It is not a good invariant metric because of its dependency.\n",
    "\n",
    "Since the goal of the change is to reduce the time coaches spend helping non-paying students I would look at both **Gross conversion** and **Net conversion** as it has direct connection to both cost and revenue. A successful experiment should show a decrease in Gross conversion because of better expectation settings and an increase in Net conversion because the students that choose to continue are more likely to be successful and continue past the 14-day trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Standard Deviation\n",
    "\n",
    "##### Final Project Baseline Values:\n",
    "* Unique cookies to view page per day: **40000**\n",
    "* Unique cookies to click \"Start free trial\" per day: **3200**\n",
    "* Enrollments per day: **660**\n",
    "* Click-through-probability on \"Start free trial\": **0.08**\n",
    "* Probability of enrolling, given click: **0.20625**\n",
    "* Probability of payment, given enroll: **0.53**\n",
    "* Probability of payment, given click: **0.1093125**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked thru count from 5000 samples:\n",
      "400.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 5000 sample cookies\n",
    "sample_size = 5000\n",
    "\n",
    "# click_thru_probability of 5000 samples\n",
    "clicked_thru = sample_size * 0.08\n",
    "print \"Clicked thru count from 5000 samples:\"\n",
    "print clicked_thru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD of Gross conversion:\n",
      "0.0202\n"
     ]
    }
   ],
   "source": [
    "# SD of Gross conversion\n",
    "p = 0.20625\n",
    "sd_gc = round(np.sqrt(p * (1 - p) / clicked_thru),4)\n",
    "print \"SD of Gross conversion:\"\n",
    "print sd_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD of Net Conversion:\n",
      "0.0156\n"
     ]
    }
   ],
   "source": [
    "# SD of Net Conversion\n",
    "p = 0.1093125\n",
    "sd_nc = round(np.sqrt(p * (1 - p) / clicked_thru),4)\n",
    "print \"SD of Net Conversion:\"\n",
    "print sd_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard deviation of evaluation metrics:\n",
    "* **Gross conversion:** 0.0202\n",
    "* **Net conversion:** 0.0156\n",
    "\n",
    "##### For each of your evaluation metrics, indicate whether you think the analytic estimate would be comparable to the empirical variability, or whether you expect them to be different (in which case it might be worth doing an empirical estimate if there is time). Briefly give your reasoning in each case.\n",
    "\n",
    "I would expect that the analytic estimate would be comparable to the empirical variability because Gross Conversion and Net Conversion use the Unit of Diversion (number of unique cookies) as the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizing\n",
    "\n",
    "### Number of Samples vs. Power\n",
    "##### Indicate whether you will use the Bonferroni correction during your analysis phase, and give the number of pageviews you will need to power you experiment appropriately.\n",
    "\n",
    "* Bonferroni correction will not be used during the analysis phase.\n",
    "\n",
    "* **Gross Conversion -**\n",
    " * Baseline: 0.20625\n",
    " * dmin: 0.01\n",
    " * alpha = 0.05\n",
    " * beta = 0.2\n",
    " * **Sample Size:** 25,835\n",
    "\n",
    "* **Net Conversion -**\n",
    " * Baseline: 0.1093125\n",
    " * dmin: 0.0075\n",
    " * alpha = 0.05\n",
    " * beta = 0.2\n",
    " * ** Sample Size:** 27,413\n",
    " \n",
    "Using the sample size for Net Conversion (because it is the higher of the two sample sizes), multiplied by 2 for each group (control + experiment), then divided by the baseline for Click-through-probability on \"Start free trial\" to calculate the number of pageviews needed:\n",
    "\n",
    "(27,413 * 2) / 0.08 = 685,325\n",
    "\n",
    "**685,325** pageviews will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration vs. Exposure\n",
    "##### Indicate what fraction of traffic you would divert to this experiment and, given this, how many days you would need to run the experiment. \n",
    "Based on the risk analysis below, **70%** of traffic would be diverted (28,000 unique cookies per day based on the provided baseline of 40,000). To get the needed 685,325 pageviews, the experiment would need to run for **25 days**.\n",
    "\n",
    "##### Give your reasoning for the fraction you chose to divert. How risky do you think this experiment would be for Udacity?\n",
    "The changes needed to implement the experiment are fairly simple so the chance of bugs is low. Having said that, since the change affects the new enrollment process, diverting all traffic is too risky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sanity Checks\n",
    "##### For each of your invariant metrics, give the 95% confidence interval for the value you expect to observe, the actual observed value, and whether the metric passes your sanity check.\n",
    "\n",
    "* **Number of cookies:**\n",
    " * 95% confidence interval: **0.4988, 0.5012**\n",
    " * Observed Value: **0.5006**\n",
    " * Sanity Check: **Passed**\n",
    " \n",
    "* **Number of clicks:**\n",
    " * 95% confidence interval: **0.4959, 0.5041**\n",
    " * Observed Value: **0.5005**\n",
    " * Sanity Check: **Passed**\n",
    " \n",
    "* **Click-through-probability:**\n",
    " * 95% confidence interval: **0.0812, 0.0830**\n",
    " * Observed Value: **0.0822**\n",
    " * Sanity Check: **Passed**\n",
    "\n",
    "##### For any sanity check that did not pass, explain your best guess as to what went wrong based on the day-by-day data. Do not proceed to the rest of the analysis unless all sanity checks pass.\n",
    " \n",
    "All sanity checks passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Result Analysis\n",
    "\n",
    "### Effect Size Tests\n",
    "For each of your evaluation metrics, give a 95% confidence interval around the difference between the experiment and control groups. Indicate whether each metric is statistically and practically significant.\n",
    "\n",
    "* **Gross conversion:**\n",
    "  * 95% confidence interval: **-0.0291, -0.0120**\n",
    "  * Statistically Significant: **Yes**\n",
    "  * practically significant: **Yes**\n",
    "  \n",
    "* **Net conversion:**\n",
    "  * 95% confidence interval: **-0.0116, 0.0019**\n",
    "  * Statistically Significant: **No**\n",
    "  * practically significant: **No**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign Tests\n",
    "##### For each of your evaluation metrics, do a sign test using the day-by-day data, and report the p-value of the sign test and whether the result is statistically significant.\n",
    "\n",
    "* **Gross conversion:**\n",
    "  * p-value: **0.0026**\n",
    "  * Statistically Significant: **Yes**\n",
    "\n",
    "* **Net conversion:**\n",
    "  * p-value: **0.6776**\n",
    "  * Statistically Significant: **No**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "##### State whether you used the Bonferroni correction, and explain why or why not. If there are any discrepancies between the effect size hypothesis tests and the sign tests, describe the discrepancy and why you think it arose.\n",
    "\n",
    "In this case, because the experiment is testing only a single variation, Bonferroni correction was not used. There were no discrepancies between the effect size hypothesis tests and the sign tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "##### Make a recommendation and briefly describe your reasoning.\n",
    "\n",
    "The negative and the practically and statistically significant results of the Gross Conversion indicate that the change is discouraging users from signing up by letting the users know how much time they would need to invest per week in order to be successful and less users are signing up for the free trial. This means that Udacity can potentially save money by limiting the amount of time the coaches spend helping non-paying students.\n",
    "\n",
    "That being said, although the Net Conversion came out as practically and statistically insignificant, the fact that the lower bounds of the 95% confidence interval are a negative number may indicate a potential for loss of revenue, perhaps from users that were discouraged from signing up but would have otherwise paid at least one payment.\n",
    "\n",
    "I recommend that additional analysis is performed since further evaluation is needed in order to make a decision, including evaluating the potential savings from reducing time spent with non-paying users against the potential loss of revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-Up Experiment\n",
    "##### Give a high-level description of the follow up experiment you would run, what your hypothesis would be, what metrics you would want to measure, what your unit of diversion would be, and your reasoning for these choices.\n",
    "\n",
    "For a follow up experiment I would recommend looking at the number of payments made by each student. The same results dataset can be used as long as Udacity can link the experiment results back to the specific users and retrieve the number and amount of all payments they made after registration.\n",
    "\n",
    "Even though the results of this experiment showed that there was no significant change in in the Net Conversion value, I think that a deeper look will reveal that by implementing this change, Udacity will be able to focus its resources more effectively and increase it's per user margin. In addition to the financial benefits, student satisfaction and rate of success may also improve, along with a reduction in negative reviews. \n",
    "\n",
    "My **hypothesis** is that using the same experiment (i.e. implementing the same 5-hour prompt message) may result in an equal or lower number of users making at least one payment, but the overall revenue generated by these users would be higher as they are more likely to make more than a single payment. This type of analysis will help Udacity evaluate not just initial potential for revenue but also compare the overall worth of the users over time. The experiment itself would be the same but results would only be available after a longer period of time so enough data is available for analysis.\n",
    "\n",
    "The **unit of diversion** will be the number of payments per user-id and the **evaluation metrics** will be the count of those payments after specific time intervals (for example, number of payments at 3, 6, 9 and 12 months’ post-registration) so we can evaluate if the change between the control and experiment group is statistically and practically significant. The **invariant metrics** should remain as they are now since we are performing the same experiment (or even using the same dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
