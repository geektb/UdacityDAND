{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5 - Identifying Fraud from Enron Emails\n",
    "\n",
    "\n",
    "##### Question 1(a) - Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question.\n",
    "\n",
    "The goal of this project is to evaluate internal Enron e-mails of 140+ individuals and financial data related to their earning to try and identify a pattern or correlation between the e-mail and financial data and whether an individual was considered a Person of Interest (POI) in the Enron investigation.\n",
    "\n",
    "Udacity classified individuals in the financial dataset as a Person of Interest based on the following criteria:\n",
    "\n",
    "- Individuals who were indicted\n",
    "- Individuals who reached a settlement or plea deal with the government\n",
    "- Individuals who testified in exchange for prosecution immunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment -  \n",
    "The first step I took was to setup my coding environment by loading the needed python libraries, populate the features_list variable with a complete list of available features and load the data into a data_dict object. I intentionally started with a complete list of available features (excluding the non-numeric values of email_address) so I can use different variations when choosing the best features (see Question 2).\n",
    "\n",
    "In addition, I created a function called get_kbest_features that accepts a dataframe and an integer number and returns a list of features using SelectKBest and the integer as the value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn import tree\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from tester import test_classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "\n",
    "def get_kbest_features(df, k):\n",
    "    sel_kbest = SelectKBest(f_regression, k)\n",
    "    sel_kbest.fit(df, df['poi'])\n",
    "    features_SelectKBest = df.columns[sel_kbest.get_support()].tolist()\n",
    "    return features_SelectKBest\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "features_list = ['poi', 'bonus', 'deferral_payments', 'deferred_income', 'director_fees', \n",
    "                 'exercised_stock_options', 'expenses', 'from_messages', 'from_poi_to_this_person', \n",
    "                 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', \n",
    "                 'restricted_stock', 'restricted_stock_deferred', 'salary', 'shared_receipt_with_poi', \n",
    "                 'to_messages', 'total_payments', 'total_stock_value']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Data Exploration -  \n",
    "Finding out how many Persons Of Interest, Data Points and Features we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of POIs:  18\n",
      "# of Data points:  146\n",
      "# of Features:  21\n"
     ]
    }
   ],
   "source": [
    "data_points_count = len(data_dict)\n",
    "data_features_count = len(data_dict[data_dict.keys()[0]])\n",
    "\n",
    "poi_count = 0\n",
    "for value in data_dict.values():\n",
    "    if value['poi'] == 1:\n",
    "        poi_count += 1\n",
    "\n",
    "print \"# of POIs: \", poi_count\n",
    "print \"# of Data points: \", data_points_count\n",
    "print \"# of Features: \", data_features_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Outlier Investigation -  \n",
    "\n",
    "##### Question 1(b) - Were there any outliers in the data when you got it, and how did you handle those?\n",
    "By plotting the available features (see sample plot of Salary and Bonus below) I was able to identify an outlier that existed across all features called \"TOTAL\". This was clearly a total of all the values of data points and I decided to remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEVCAYAAADOwrOnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGLpJREFUeJzt3X2QZXV95/H3Z4SBQUSDjEgm2iMiCxomStWOJFKhEYlD\nYsRSN4IWKioZdTHZysaAT+Vo3FLKqmyiLiI6YcXoSJRNHB9QyEonIYhM5NE4I6DMgGCw41OcCSzj\n+N0/7pmxbfrh9pnuPvcO71fVrb7nnt8953sPTX/m9/udc26qCkmS5mpJ1wVIkoaTASJJasUAkSS1\nYoBIkloxQCRJrRggkqRW9rkASbI+yX1Jbumj7ROSfDnJDUluSnLaYtQoSfuCfS5AgEuA5/bZ9q3A\nZVV1PHAmcOGCVSVJ+5h9LkCq6hrghxNfS3JkkiuSbEry90mOblb9DDikef4Y4J5FLFWShtp+XRew\nSC4G1lbVt5KsBj4InAK8A7gyyR8ABwHP6bBGSRoq+3yAJHkk8BvAp5KkeXn/5ueZwCVV9T+TnAD8\nFfC0DsqUpKGzzwcIvWG6HzbzHJO9mma+pKquS3JgksOq6t8WtUJJGkKdz4HMdtZUkpcmubl5XJPk\nuH422zyoqp8AdyZ58YRtrmqebqMZtkpyLHCA4SFJ/ek8QJj9rKlvA79ZVb8GvAv48EwbS/IJ4Frg\n6CR3JTkbeBnw6uZU3a8Dz2+a/zFwTpKbgI8Dr9i7jyJJDx8ZhNu5JxkBPltVq2Zp9xjg1qp6wuJU\nJkmaziD0QObiNcAVXRchSRqiSfQkJwNnAyd2XYskaUgCpJn0vhhYU1U/nKFd9+NxkjRkqiqzt3qo\nQRnC2nPW1ENWJE8ELgfOqqpvzbahqhrKx9vf/vbOa7D+7uuw/uF8DHP9e6PzHkhz1tQo8NgkdwFv\nB5YCVVUXA28DDgUubC4E3FlVq7uqV5LU03mAVNVLZ1l/DnDOIpUjSerToAxhPeyNjo52XcJesf5u\nWX+3hr3+tgbiOpD5kqT2pc8jSQstCTXkk+iSpCFjgEiSWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS\n1IoBIklqxQCRJLVigEiSWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCRJLVigEiSWjFA\nJEmtGCCSpFYMEElSK50HSJL1Se5LcssMbd6X5PYkNyV5+mLWJ0maWucBAlwCPHe6lUlOA55cVU8B\n1gIXLVZhkqTpdR4gVXUN8MMZmpwOXNq0/Srw6CSHL0ZtkqTpdR4gfVgB3D1h+Z7mNUlSh/bruoD5\ntm7duj3PR0dHGR0d7awWSRo0Y2NjjI2Nzcu2UlXzsqG9KiIZAT5bVaumWHcRcHVVXdYsbwFOqqr7\npmhbg/B5JGlYJKGq0ua9gzKEleYxlY3AywGSnAD8aKrwkKRhND4+zqZNmxgfH++6lDnrPECSfAK4\nFjg6yV1Jzk6yNsnvA1TVF4A7k9wBfAh4fYflStK82bDhMkZGjuHUU1/LyMgxbNhwWdclzclADGHN\nF4ewJA2L8fFxRkaO4f77rwZWAbewbNnJbNu2heXLly9aHfvCEJYkPaxs3bqVpUtX0gsPgFXsv/8I\nW7du7a6oOTJAJKkDK1eu5MEHtwK7b8JxCzt3bmPlypXdFTVHBogkdWD58uWsX38hy5adzCGHHM+y\nZSezfv2Fizp8tbecA5GkDo2Pj7N161ZWrlzZSXjszRyIASJJD2NOokuSFp0BIklqxQCRJLVigEiS\nWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCRJLVigEiSWjFAJEmtGCCSpFYMEElSKwaI\nJKkVA0SS1IoBIklqpfMASbImyZYktyU5b4r1hyTZmOSmJLcmeWUHZUqSJun0O9GTLAFuA04B7gU2\nAWdU1ZYJbd4EHFJVb0pyGPBN4PCq+ukU2/M70SVpDob5O9FXA7dX1baq2gl8Ejh9UpsCHtU8fxTw\n/anCQ5K0uLoOkBXA3ROWv9O8NtEHgKcmuRe4GfjDRapNkjSD/bouoA/PBW6sqmcneTJwVZJVVbV9\nqsbr1q3b83x0dJTR0dFFKVKShsHY2BhjY2Pzsq2u50BOANZV1Zpm+XygquqCCW0+B7y7qv6pWf6/\nwHlV9c9TbM85EEmag2GeA9kEHJVkJMlS4Axg46Q224DnACQ5HDga+PaiVilJeohOh7CqaleSc4Er\n6YXZ+qranGRtb3VdDLwL+N9Jbmne9idV9YOOSpYkNTodwppvDmFJ0twM8xCWJGlIGSCSpFYMEElS\nKwaIJKkVA0SS1IoBIklqxQCRJLVigEiSWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCR\nJLVigEiSWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCRJLXSeYAkWZNkS5Lbkpw3TZvR\nJDcm+XqSqxe7RknSQ6Wqutt5sgS4DTgFuBfYBJxRVVsmtHk0cC3wW1V1T5LDqurfptledfl5JGnY\nJKGq0ua9XfdAVgO3V9W2qtoJfBI4fVKblwKXV9U9ANOFhyRpcXUdICuAuycsf6d5baKjgUOTXJ1k\nU5KzFq06SdK09uu6gD7sBxwPPBt4JPCVJF+pqjumarxu3bo9z0dHRxkdHV2EEiVpOIyNjTE2NjYv\n2+p6DuQEYF1VrWmWzweqqi6Y0OY84MCqekez/BHgiqq6fIrtOQciSXMwzHMgm4CjkowkWQqcAWyc\n1OYzwIlJHpHkIOCZwOZFrlOSNEmnQ1hVtSvJucCV9MJsfVVtTrK2t7ourqotSb4E3ALsAi6uqm90\nWLYkiT6HsJL8F+CLVfWTJG+lNyfxrqq6YaELnAuHsCRpbhZjCOttTXicCDwHWA98sM0OJUn7hn4D\nZFfz83foDSF9Hli6MCVJkoZBvwFyT5IPAS8BvpDkgDm8V5K0D+p3DuQgYA1wa1XdnuQI4LiqunKh\nC5wL50AkaW72Zg6k3wB54lSvV9VdbXa6UAwQSZqbxQiQW4ECAhwIPAn4ZlU9rc1OF4oBIklzszcB\n0td1IFV13KQdHg+8vs0OJUn7hlYT4c31H8+c51okSUOkrx5Ikj+asLiE3oWE9y5IRZKkodDvrUwe\nNeH5T4HPAw+5maEk6eGj07vxzjcn0SVpbhZ8Ej3J0cAfAysnvqeqnt1mp5Kk4dfvabw3AxcBX+Pn\ntzWhqr62cKXNnT0QSZqbBe+BAD+tKm+eKEnao9/TeD+b5PVJjkhy6O7HglYmSRpo/Q5h3TnFy1VV\nR85/Se05hCVJc7PgtzIZFgaIJM3NYpyFtT/wOuA3m5fGgA9V1c42O5UkDb9+h7A+AuwPfLR56Sxg\nV1W9ZgFrmzN7IJI0N4txN96bq+rXZnutawaIJM3NYnwn+q4kT56wwyOZcD2IJOnhp9/rQN4IXJ3k\n283ySuDsBalIkjQU+u2B/BPwIeBnwA+a519ZqKIkSYOv3wC5lN63EP4p8H7gSOBj81FAkjVJtiS5\nLcl5M7T7z0l2JnnhfOxXkrR3+h3C+tWqeuqE5auTfGNvd55kCfAB4BR63y+yKclnqmrLFO3eA3xp\nb/cpSZof/fZAbkhywu6FJM8E/nke9r8auL2qtjXXlHwSOH2Kdm8APg18bx72KUmaBzP2QJLcChS9\na0CuTXJXszwCbJnpvX1aAdw9Yfk79EJlYg2/DLygqk5O8gvrJEndmW0I63mLUsXM/hyYODcy4/nK\n69at2/N8dHSU0dHRBSlKkobR2NgYY2Nj87KtTu+F1QyLrauqNc3y+fRu0njBhDa7Tx0OcBiwA/j9\nqto4xfa8kFCS5mBob6aY5BHAN+lNon8XuB44s6o2T9P+EuCzVfV/pllvgEjSHCzGF0otiKraleRc\n4Ep6E/rrq2pzkrW91XXx5LcsepGSpCl5O3dJehhbjHthSZL0CwwQSVIrBogkqRUDRJLUigEiSWrF\nAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKk\nVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJa6TxAkqxJsiXJbUnOm2L9S5Pc3DyuSXJcF3VK\nkn5Rqqq7nSdLgNuAU4B7gU3AGVW1ZUKbE4DNVfXjJGuAdVV1wjTbqy4/jyQNmyRUVdq8t+seyGrg\n9qraVlU7gU8Cp09sUFXXVdWPm8XrgBWLXKMkaQpdB8gK4O4Jy99h5oB4DXDFglYkSerLfl0X0K8k\nJwNnAyfO1G7dunV7no+OjjI6OrqgdUnSMBkbG2NsbGxettX1HMgJ9OY01jTL5wNVVRdMarcKuBxY\nU1XfmmF7zoFI0hwM8xzIJuCoJCNJlgJnABsnNkjyRHrhcdZM4SFJWlydDmFV1a4k5wJX0guz9VW1\nOcna3uq6GHgbcChwYZIAO6tqdXdVS5Kg4yGs+eYQliTNzTAPYUmShpQBIklqxQCRJLVigEiSWjFA\nJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCRJLVigAyg8fFxNm3axPj4eNelSNK0DJABs2HD\nZYyMHMOpp76WkZFj2LDhsq5LkqQpeTfeATI+Ps7IyDHcf//VwCrgFpYtO5lt27awfPnyPW22bt3K\nypUr97wmSW15N959xNatW1m6dCW98ABYxf77j7B161bA3omkwWIPZIDM1AMBmnWXA48EdrBs2Yt+\noXciSXNlD2Qf8uY3/3cOPPAkDjnkeJYtO5n16y9k+fLlTS/kMcCLgNcCL6LqkD29E0labJ1+pa1+\nbsOGy3jVq17LIx5xOABvfOOLWbv2nD29i4MPPpj77/8ucB27eycPPHACBx98cGc1S3p4swcyAMbH\nx3nFK87hgQfCjh2P5IEHwjvf+e5faLN9+3aWLTuKifMjy5Y9me3bty96vZIEBshAuPHGG9m5cxcw\nBnwNGGPnzp/x4Q9/eM+1ICtXrgTuAW5p3nULcG/zuiQtPgNkYPwyvd7FOPD/gMfxlre8kxUrjmTD\nhstYvnw569dfyLJlJz9kfkSSuuBZWANgfHycFSuOYufOtwAXAE8AbgNeD/wlBx5Y3HXXbSxfvpzN\nmzdz/fXXs3r1ao499thO65Y0/PbmLCwDZECcfvoL2LjxSnqT5PsDnwb+B/BYDjroYMbG/oo77vg2\nr37161m6dCUPPriV9esv5MwzX9Jp3ZKG21AHSJI1wJ/TG05bX1UXTNHmfcBpwA7glVV10zTbGsoA\nGR8f54gjnsSuXY8HjgL+DjgC+D7wALAf11xzNaee+vwZr1KXpLka2utAkiwBPgA8F3gacGaSYya1\nOQ14clU9BVgLXLTohS6wz3zmM+zatQO4E/gH4Eh64XE8cCBwMM9+9mnACqa7Sl2SFlvXk+irgdur\naltV7QQ+CZw+qc3pwKUAVfVV4NFJDl/cMhfOG97w3zjnnNfTuyTnAHpDWLc1P28ADgUe4MEHL+X+\n++9g4llYO3du8ywsSZ3p+kLCFcDdE5a/Qy9UZmpzT/PafQtb2sLbvHkzH/jARfT+M6wBvs7EHkbv\nY94JPAlYwYEHHk7VSRxwwJPYuXObZ2FJ6lTXATLv1q1bt+f56Ogoo6OjndUym+uvvx54dPN4FfB7\n9HoYq5qf9wAHAd8DdpD8OzfccC3bt2/3brySWhkbG2NsbGxettXpJHqSE4B1VbWmWT4fqIkT6Uku\nAq6uqsua5S3ASVX1kB7IsE2ib968mac+9Rn0RhKvA14GfItez+Me4FhgMwcccBhLluzwrCtJ825o\nJ9GBTcBRSUaSLAXOADZOarMReDnsCZwfTRUew+jYY4/l3HPXAjuBE4D76V1EeCe9Gydu4b3vfRf/\n+I+Xs23bFsND0kAZlNN4/4Kfn8b7niRr6fVELm7afIDeJMEO4OyqumGabQ1VD2S3zZs38+53v5ur\nrrqKHTt2MDIywtlnn81ZZ53lMJWkBTXU14HMp2ENEEnqyjAPYUmShpQBIklqxQCRJLVigEiSWjFA\nJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCRJLVigEiSWjFAJEmtGCCSpFYMEElSKwaIJKkV\nA0SS1IoBIklqxQCRJLVigEiSWjFAJEmtGCCSpFY6C5Akv5TkyiTfTPKlJI+eos2vJPlykn9JcmuS\nP+iiVknSQ3XZAzkf+Luq+k/Al4E3TdHmp8AfVdXTgF8H/muSYxaxxkUzNjbWdQl7xfq7Zf3dGvb6\n2+oyQE4HPto8/yjwgskNqupfq+qm5vl2YDOwYtEqXETD/gto/d2y/m4Ne/1tdRkgj6uq+6AXFMDj\nZmqcZCXwdOCrC16ZJGlW+y3kxpNcBRw+8SWggLdO0bxm2M7BwKeBP2x6IpKkjqVq2r/bC7vjZDMw\nWlX3JXk8cHVVHTtFu/2AzwFXVNVfzLLNbj6MJA2xqkqb9y1oD2QWG4FXAhcArwA+M027vwS+MVt4\nQPuDIEmauy57IIcCfw08AdgG/F5V/SjJEcCHq+p5SZ4F/ANwK70hrgLeXFVf7KRoSdIenQWIJGm4\nDe2V6P1ciNi025rk5iQ3Jrl+seucop41SbYkuS3JedO0eV+S25PclOTpi13jTGarP8lJSX6U5Ibm\nMdUJE51Isj7JfUlumaHNIB/7Gesf8GPf10XBg3r8+6l/wI//AUm+2vwdvDXJ26dpN7fjX1VD+aA3\nd/InzfPzgPdM0+7bwC91XW9TyxLgDmAE2B+4CThmUpvTgM83z58JXNd13XOs/yRgY9e1TlP/ifRO\nBb9lmvUDe+z7rH+Qj/3jgac3zw8Gvjlkv/v91D+wx7+p76Dm5yOA64DVe3v8h7YHQh8XIjbC4PS0\nVgO3V9W2qtoJfJLe55jodOBSgKr6KvDoJIczGPqpH3rHfOBU1TXAD2doMsjHvp/6YXCPfT8XBQ/s\n8e+zfhjQ4w9QVf/RPD2A3glUk+cv5nz8B+UPaxv9XohYwFVJNiU5Z9Gqm9oK4O4Jy9/hob+Ek9vc\nM0WbrvRTP8CvN13gzyd56uKUNi8G+dj3a+CP/QwXBQ/F8Z/louaBPf5JliS5EfhX4Kqq2jSpyZyP\nf5en8c5qni5EfFZVfTfJcnpBsrn5l5wWxteAJ1bVfyQ5Dfhb4OiOa3q4GPhjP+wXBc9S/0Af/6r6\nGfCMJIcAf5vkqVX1jb3Z5kD3QKrq1KpaNeFxXPNzI3Df7u5VcyHi96bZxnebn+PA39AbhunKPcAT\nJyz/SvPa5DZPmKVNV2atv6q27+4qV9UVwP7NKdvDYJCP/awG/dg3FwV/GvhYVU113ddAH//Z6h/0\n479bVf07cDWwZtKqOR//gQ6QWey+EBGmuRAxyUHNvxhI8kjgt4CvL1aBU9gEHJVkJMlS4Ax6n2Oi\njcDLAZKcAPxo91DdAJi1/oljpklW0ztV/AeLW+aMwvTj1IN87Hebtv4hOPazXRQ86Md/xvoH+fgn\nOWz3mapJlgGnAlsmNZvz8R/oIaxZXAD8dZJX0VyICDDxQkR6w19/k94tTvYDPl5VV3ZVcFXtSnIu\ncCW98F5fVZuTrO2trour6gtJfjvJHcAO4Oyu6p2sn/qBFyd5HbATuB94SXcV/6IknwBGgccmuQt4\nO7CUITj2MHv9DPaxfxbwMuDWZhy+gDfTO6Nv4I9/P/UzwMcfOAL4aJIl9P7fvaw53nv1t8cLCSVJ\nrQzzEJYkqUMGiCSpFQNEktSKASJJasUAkaQh1c8NQie0/bPmZoo3pHcT2r0+xdizsCRpSCU5EdgO\nXFpVq+bwvnPp3RzyNXuzf3sg0gJIckmSF3Zdh/ZtU91gM8mRSa5o7v/390mmup3KmcCGvd3/MF9I\nKO0zkjyiqnZ1XYf2CRcDa6vqW80V8R8ETtm9MskTgZXAl/d2RwaI1KckB9H7GuYV9L5T4U+BY4Df\nBQ4Erq2q107xvrcBzwOWTWyT5Gp636nyLOBzSV4JPKW54v9RwM27lxf6s2nf0Nyy6TeATyXZfcub\n/Sc1OwP4dM3D/IVDWFL/1gD3VNUzmvHmLwLvr6rVzfJBSX5nive9v6qeOU2b/Zv3v5PeDe52rzsD\nuNzw0BwtAX5YVcc3v6fPqKpfndTmDOZh+Gr3ziT151bg1CTvTnJiVf0EOCXJdc1ZMCcDT5vifTO1\nuWzC8/X8/P5DZwOXzP9H0D5ozw02m9/JO5O8eM/KZNWE58cAj6mq6+ZjxwaI1Kequh04nl6Q/Gkz\nNPW/gBc2vYuP0BvK2iPJAbO02TFh+9cCK5OcBCzZ2+9q0L6vucHmtcDRSe5Kcja9mz6+uvliq68D\nz5/wlpfQ+ybReeEciNSn5k7PP6iqTyT5MfAaendl/UHztQEvBj416W0HNm2+P0ObiT4GfAJ4x3zX\nr31PVb10mlWnTdN+Xn+vDBCpf8cB703yM+BB4HXAC+h9x8x3gesntC2Aqvpxko8A/zJdm0k+Tm9y\nft7+lSgtFC8klAZIM3b9u1X1iq5rkWZjD0QaEEneR+9Mr9/uuhapH/ZAJEmteBaWJKkVA0SS1IoB\nIklqxQCRJLVigEiSWjFAJEmt/H+VN9yEGJOPDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xad8d4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "### Identifying outlier by plotting values.\n",
    "\n",
    "# testing for outliers visually with Salary and Bonus data\n",
    "outlier_features = [\"salary\", \"bonus\"]\n",
    "outlier_data = featureFormat(data_dict, outlier_features)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(outlier_data[:,0], outlier_data[:,1])\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "\n",
    "# convert to Dataframe to identify outlier record\n",
    "outlier_df = pd.DataFrame(data_dict)\n",
    "\n",
    "# show \"person\" with highest salary/bonus values that is skewing our data\n",
    "print outlier_df.loc['salary',:].idxmax(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xaddc278>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEVCAYAAADD3MPgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUVERCKlxCIiIpFSYhERkUj9f78Q1MwjmfjjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xad9fcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Found and removed a row for TOTAL that seems to be the outlier \n",
    "### across all values as it represents the total line in the report.\n",
    "data_dict.pop('TOTAL', 0)\n",
    "\n",
    "# reload outlier_data after TOTAL removal\n",
    "outlier_data = featureFormat(data_dict, outlier_features)\n",
    "\n",
    "# Plot again, now showing a more reasonable plot\n",
    "plt.scatter(outlier_data[:,0], outlier_data[:,1])\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Creating New Features -\n",
    "\n",
    "##### As part of the assignment, you should attempt to engineer your own feature that does not come readymade in the dataset explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.)\n",
    "\n",
    "I created 4 new features:\n",
    "- **total_pay:** Sum of each Data Point's Total Stock Value and Total Payments. The reason I created this feature is to check for any correlation between the overall amount of money a person received and their likelihood of being a POI.\n",
    "- **salary_total_pay_ratio:** This is the portion of a person's salary in their overall compensation they received. I created this feature to evaluate of POIs seem to receive more compensation that is not their base salary that other individuals. I chose to use a ratio so we can get a sense of the proportions regardless of the overall compensation.\n",
    "- **from_poi_ratio:** This is the ratio of e-mails received from POIs as a portion of all e-mails received by a person. I chose to use a ratio to standardize values across data points regardless of how many e-mails this person received overall. \n",
    "- **to_poi_ratio:** This is the ratio of e-mails sent to POIs as a portion of all e-mails sent by a person. I chose to use a ratio to standardize values across data points regardless of how many e-mails this person sent overall.\n",
    "\n",
    "The reason I chose the last two features is so our testing is aligned with the project overall purpose of using the e-mails data set to find any correlation between a person's likelihood of being a POI and any features we are able to collect from the e-mails data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "df = pd.DataFrame(data, columns = ['poi',\n",
    "                 'bonus',\n",
    "                 'deferral_payments',\n",
    "                 'deferred_income',\n",
    "                 'director_fees',\n",
    "                 #'email_address', #excluding non-numeric value\n",
    "                 'exercised_stock_options',\n",
    "                 'expenses',\n",
    "                 'from_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'loan_advances',\n",
    "                 'long_term_incentive',\n",
    "                 'other',\n",
    "                 'restricted_stock',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'salary',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'to_messages',\n",
    "                 'total_payments',\n",
    "                 'total_stock_value'\n",
    "                ])\n",
    "\n",
    "df['total_pay'] = df['total_stock_value'] + df['total_payments']\n",
    "df['salary_total_pay_ratio'] = df['salary'] / df['total_pay']\n",
    "df['from_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['to_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "\n",
    "# replacing all NaN with 0\n",
    "df = df.replace({np.nan: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest Scores of All Features:\n",
    "As you can see below, the new 'total_pay' and 'to_poi_ratio' features are in the top 10 SelectKBest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 6.39511147087e+17  :  poi\n",
      "2. 25.0975415287  :  exercised_stock_options\n",
      "3. 24.4676540475  :  total_stock_value\n",
      "4. 21.0600017075  :  bonus\n",
      "5. 18.575703268  :  salary\n",
      "6. 17.187006077  :  total_pay\n",
      "7. 16.6417070705  :  to_poi_ratio\n",
      "8. 11.5955476597  :  deferred_income\n",
      "9. 10.0724545294  :  long_term_incentive\n",
      "10. 9.34670079105  :  restricted_stock\n",
      "11. 8.86672153711  :  total_payments\n",
      "12. 8.74648553213  :  shared_receipt_with_poi\n",
      "13. 7.24273039654  :  loan_advances\n",
      "14. 6.23420114051  :  expenses\n",
      "15. 5.34494152315  :  from_poi_to_this_person\n",
      "16. 4.2049708583  :  other\n",
      "17. 3.21076191697  :  from_poi_ratio\n",
      "18. 2.42650812724  :  from_this_person_to_poi\n",
      "19. 2.10765594328  :  director_fees\n",
      "20. 1.69882434858  :  to_messages\n",
      "21. 0.21705893034  :  deferral_payments\n",
      "22. 0.164164498234  :  from_messages\n",
      "23. 0.0649843117237  :  restricted_stock_deferred\n",
      "24. 0.00255088867495  :  salary_total_pay_ratio\n"
     ]
    }
   ],
   "source": [
    "feature_scores_kbest = SelectKBest(f_regression, k='all').fit(df, df['poi'])\n",
    "feature_scores = sorted(enumerate(feature_scores_kbest.scores_),key=lambda x:x[1], reverse=True)[:1000]\n",
    "feature_scores_indices = map(list,zip(*feature_scores))[0]\n",
    "i = 1\n",
    "for feature_pvalue in feature_scores:\n",
    "    print '{3}. {2}  :  {1}'.format(feature_pvalue[0], df.columns[feature_pvalue[0]], feature_pvalue[1], i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of New Features on Performance:\n",
    "\n",
    "Jumping ahead a bit, it turns out that the best performance I got was using the Gaussian NB CLassifier and the top 8 features from SelectKBest ('poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio'). \n",
    "        \n",
    "        Accuracy: 0.86513\tPrecision: 0.49235\tRecall: 0.37000\tF1: 0.42250\tF2: 0.38935\n",
    "\n",
    "In comparrison, excluding the two new features I created that were included previously, 'total_pay' and 'to_poi_ratio', returned the following scores:\n",
    "\n",
    "        Accuracy: 0.85464\tPrecision: 0.48876\tRecall: 0.38050\tF1: 0.42789\tF2: 0.39814\n",
    "        \n",
    "Comparing the two results, it seems that the new features I added didn't have much of an impact on the performance of the GaussianNB Classifier. Accuracy and Precision were a bit better using the two new features, but Recall and F1 scores actually improved without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.85464\tPrecision: 0.48876\tRecall: 0.38050\tF1: 0.42789\tF2: 0.39814\n",
      "\tTotal predictions: 14000\tTrue positives:  761\tFalse positives:  796\tFalse negatives: 1239\tTrue negatives: 11204\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print test_classifier(GaussianNB(), df.to_dict(orient='index'), ['poi', 'bonus', 'deferred_income', \n",
    "                                        'exercised_stock_options', 'salary', \n",
    "                                        'total_stock_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is KBest really Best?\n",
    "\n",
    "The test above made me wonder about the impact the two new features have on performance individually so I ran the same validation twice more, each time omitting only one of the two new features. What I found was that removing to_poi_ratio had no impact on the scores and the results were identical with and without this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86513\tPrecision: 0.49235\tRecall: 0.37000\tF1: 0.42250\tF2: 0.38935\n",
      "\tTotal predictions: 15000\tTrue positives:  740\tFalse positives:  763\tFalse negatives: 1260\tTrue negatives: 12237\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print test_classifier(GaussianNB(), df.to_dict(orient='index'), ['poi', 'bonus', 'deferred_income', \n",
    "                                        'exercised_stock_options', 'salary', \n",
    "                                        'total_stock_value', 'total_pay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, removing total_pay had fairly significant impact. The scores of Precision, Recall, F1 and F2 have all gone up. In fact, Accuracy was the only score that decreased which made me wonder about my process of feature selection. Having said that, I did notice that the total predictions made with our k=8 features was 15000, yet the validation I ran using the same feature but without total_pay made only 14000 total predictions. An increase of 7.1% in the number of predictions seems significant to me, so after considering the difference in the number of total predictions between the two tests I decided to go forward with the original k=8 feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86050\tPrecision: 0.51572\tRecall: 0.38550\tF1: 0.44120\tF2: 0.40600\n",
      "\tTotal predictions: 14000\tTrue positives:  771\tFalse positives:  724\tFalse negatives: 1229\tTrue negatives: 11276\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print test_classifier(GaussianNB(), df.to_dict(orient='index'), ['poi', 'bonus', 'deferred_income', \n",
    "                                        'exercised_stock_options', 'salary', \n",
    "                                        'total_stock_value', 'to_poi_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Feature Scaling -\n",
    "\n",
    "Feature scaling was not performed because none of the Classifiers I chose to use do not require it. Firstly, the GaussianNB Classifier performs feature scaling as part of its normal behavior. In addition, the DecisionTree Classifier also doesn't require feature scaling because its algorithm doesn't evaluate Euclidean distance. The third Classifier I chose, ADA Boost, also doesn't require any feature scaling because it uses the DecisionTree classifier by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Feature Selection -\n",
    "\n",
    "Once the new features have been created, I needed to standardize all the **NaN** values and change them to 0 so our dataset is consistent since our original dataset went through a similar process using the FeatureFormat function. In addition, to get a sense of which of the features have the most amount of data in our dataset, I reviewed a list of each feature and their counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                           18\n",
       "bonus                         81\n",
       "deferral_payments             38\n",
       "deferred_income               48\n",
       "director_fees                 16\n",
       "exercised_stock_options      101\n",
       "expenses                      94\n",
       "from_messages                 86\n",
       "from_poi_to_this_person       74\n",
       "from_this_person_to_poi       66\n",
       "loan_advances                  3\n",
       "long_term_incentive           65\n",
       "other                         92\n",
       "restricted_stock             109\n",
       "restricted_stock_deferred     17\n",
       "salary                        94\n",
       "shared_receipt_with_poi       86\n",
       "to_messages                   86\n",
       "total_payments               124\n",
       "total_stock_value            125\n",
       "total_pay                    142\n",
       "salary_total_pay_ratio        94\n",
       "from_poi_ratio                74\n",
       "to_poi_ratio                  66\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting datapoints per feature\n",
    "df.replace({0: np.nan}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df.to_dict(orient='index')\n",
    "\n",
    "### Originally started with 146 data points. \n",
    "### 1 data point was removed when we identified the outlier, \n",
    "### the second row was removed in the conversion to/from the DataFrame\n",
    "### due to all of it's values being zero.\n",
    "\n",
    "print len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not?  In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.\n",
    "\n",
    "Using the get_kbest_features function I created, I tested each one of the three Classifiers I decided to use (Gaussian NB, Decision Tree, and ADA Boost) using the test_classifier function provided in tester.py using the feature selection from SelectKBest where k was every integer between 2 and the total count of features.\n",
    "\n",
    "The reason I tested multiple variations of features against each one of the classifiers I chose was to make sure that I'm using the best list of features for each classifier instead of choosing a single list of SelectKBest features and using it with all the classifiers. As a result, I found that when evaluating for the highest possible F1 value, Gaussian NB performed the best using a list of 8 features, but both the Decision Tree and ADA Boost classifiers performed better with only a list of 4 features.\n",
    "\n",
    "I focused on the F1 score was because it considers both the precision and recall scores and not just accuracy. I felt that this was a better indicator because the accuracy score can be biased due to our small data set. This is evident when reviewing the results of the Gaussian NB below that shows the highest accuracy achieved was using only the 'poi' and 'exercised_stock_options' features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Picking an Algorithm -\n",
    "\n",
    "##### 3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SelectKBest k=2\n",
      "used features: ['poi', 'exercised_stock_options']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.90409\tPrecision: 0.46055\tRecall: 0.32100\tF1: 0.37831\tF2: 0.34171\n",
      "\tTotal predictions: 11000\tTrue positives:  321\tFalse positives:  376\tFalse negatives:  679\tTrue negatives: 9624\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=3\n",
      "used features: ['poi', 'exercised_stock_options', 'total_stock_value']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84069\tPrecision: 0.46889\tRecall: 0.26750\tF1: 0.34066\tF2: 0.29264\n",
      "\tTotal predictions: 13000\tTrue positives:  535\tFalse positives:  606\tFalse negatives: 1465\tTrue negatives: 10394\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=4\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'total_stock_value']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84300\tPrecision: 0.48581\tRecall: 0.35100\tF1: 0.40755\tF2: 0.37163\n",
      "\tTotal predictions: 13000\tTrue positives:  702\tFalse positives:  743\tFalse negatives: 1298\tTrue negatives: 10257\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=5\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84677\tPrecision: 0.50312\tRecall: 0.32300\tF1: 0.39342\tF2: 0.34791\n",
      "\tTotal predictions: 13000\tTrue positives:  646\tFalse positives:  638\tFalse negatives: 1354\tTrue negatives: 10362\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=6\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86020\tPrecision: 0.46372\tRecall: 0.31000\tF1: 0.37159\tF2: 0.33201\n",
      "\tTotal predictions: 15000\tTrue positives:  620\tFalse positives:  717\tFalse negatives: 1380\tTrue negatives: 12283\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=7\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86020\tPrecision: 0.46372\tRecall: 0.31000\tF1: 0.37159\tF2: 0.33201\n",
      "\tTotal predictions: 15000\tTrue positives:  620\tFalse positives:  717\tFalse negatives: 1380\tTrue negatives: 12283\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=8\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86513\tPrecision: 0.49235\tRecall: 0.37000\tF1: 0.42250\tF2: 0.38935\n",
      "\tTotal predictions: 15000\tTrue positives:  740\tFalse positives:  763\tFalse negatives: 1260\tTrue negatives: 12237\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=9\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.85707\tPrecision: 0.45414\tRecall: 0.35650\tF1: 0.39944\tF2: 0.37252\n",
      "\tTotal predictions: 15000\tTrue positives:  713\tFalse positives:  857\tFalse negatives: 1287\tTrue negatives: 12143\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=10\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.85873\tPrecision: 0.46274\tRecall: 0.36950\tF1: 0.41090\tF2: 0.38502\n",
      "\tTotal predictions: 15000\tTrue positives:  739\tFalse positives:  858\tFalse negatives: 1261\tTrue negatives: 12142\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=11\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.85073\tPrecision: 0.42610\tRecall: 0.34450\tF1: 0.38098\tF2: 0.35822\n",
      "\tTotal predictions: 15000\tTrue positives:  689\tFalse positives:  928\tFalse negatives: 1311\tTrue negatives: 12072\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=12\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84860\tPrecision: 0.41763\tRecall: 0.34350\tF1: 0.37695\tF2: 0.35614\n",
      "\tTotal predictions: 15000\tTrue positives:  687\tFalse positives:  958\tFalse negatives: 1313\tTrue negatives: 12042\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=13\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83333\tPrecision: 0.35448\tRecall: 0.30450\tF1: 0.32760\tF2: 0.31334\n",
      "\tTotal predictions: 15000\tTrue positives:  609\tFalse positives: 1109\tFalse negatives: 1391\tTrue negatives: 11891\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=14\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83453\tPrecision: 0.35757\tRecall: 0.30250\tF1: 0.32774\tF2: 0.31211\n",
      "\tTotal predictions: 15000\tTrue positives:  605\tFalse positives: 1087\tFalse negatives: 1395\tTrue negatives: 11913\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=15\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83480\tPrecision: 0.35858\tRecall: 0.30300\tF1: 0.32846\tF2: 0.31269\n",
      "\tTotal predictions: 15000\tTrue positives:  606\tFalse positives: 1084\tFalse negatives: 1394\tTrue negatives: 11916\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=16\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.82913\tPrecision: 0.33794\tRecall: 0.29350\tF1: 0.31416\tF2: 0.30143\n",
      "\tTotal predictions: 15000\tTrue positives:  587\tFalse positives: 1150\tFalse negatives: 1413\tTrue negatives: 11850\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=17\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.82913\tPrecision: 0.33794\tRecall: 0.29350\tF1: 0.31416\tF2: 0.30143\n",
      "\tTotal predictions: 15000\tTrue positives:  587\tFalse positives: 1150\tFalse negatives: 1413\tTrue negatives: 11850\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=18\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83000\tPrecision: 0.34049\tRecall: 0.29350\tF1: 0.31525\tF2: 0.30183\n",
      "\tTotal predictions: 15000\tTrue positives:  587\tFalse positives: 1137\tFalse negatives: 1413\tTrue negatives: 11863\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=19\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.79127\tPrecision: 0.26700\tRecall: 0.32400\tF1: 0.29275\tF2: 0.31073\n",
      "\tTotal predictions: 15000\tTrue positives:  648\tFalse positives: 1779\tFalse negatives: 1352\tTrue negatives: 11221\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=20\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'to_messages', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.78480\tPrecision: 0.24502\tRecall: 0.29500\tF1: 0.26770\tF2: 0.28344\n",
      "\tTotal predictions: 15000\tTrue positives:  590\tFalse positives: 1818\tFalse negatives: 1410\tTrue negatives: 11182\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a variety of classifiers\n",
    "\n",
    "### testing results of GaussianNB against all feature combinations using SelectKBest\n",
    "for i in range(2, (len(data_dict[data_dict.keys()[0]]))):\n",
    "    print '\\nSelectKBest k=' + str(i)\n",
    "    print 'used features: ' + str(get_kbest_features(df, i))\n",
    "    clf = GaussianNB()\n",
    "    print test_classifier(clf, my_dataset, get_kbest_features(df, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best F1 Results of GaussianNB Using Default Parameters\n",
    "\n",
    "        SelectKBest k=8\n",
    "        used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
    "        GaussianNB(priors=None)\n",
    "\n",
    "        Accuracy: 0.86513\n",
    "        Precision: 0.49235\n",
    "        Recall: 0.37000\n",
    "        F1: 0.42250\n",
    "        F2: 0.38935\n",
    "\n",
    "\n",
    "        Total predictions: 15000\n",
    "        True positives:  740\n",
    "        False positives:  763\n",
    "        False negatives: 1260\n",
    "        True negatives: 12237\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SelectKBest k=2\n",
      "used features: ['poi', 'exercised_stock_options']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.86273\tPrecision: 0.27434\tRecall: 0.31000\tF1: 0.29108\tF2: 0.30214\n",
      "\tTotal predictions: 11000\tTrue positives:  310\tFalse positives:  820\tFalse negatives:  690\tTrue negatives: 9180\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=3\n",
      "used features: ['poi', 'exercised_stock_options', 'total_stock_value']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.76677\tPrecision: 0.21617\tRecall: 0.19650\tF1: 0.20587\tF2: 0.20014\n",
      "\tTotal predictions: 13000\tTrue positives:  393\tFalse positives: 1425\tFalse negatives: 1607\tTrue negatives: 9575\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=4\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'total_stock_value']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80123\tPrecision: 0.36252\tRecall: 0.38500\tF1: 0.37342\tF2: 0.38028\n",
      "\tTotal predictions: 13000\tTrue positives:  770\tFalse positives: 1354\tFalse negatives: 1230\tTrue negatives: 9646\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=5\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.78892\tPrecision: 0.31994\tRecall: 0.33050\tF1: 0.32514\tF2: 0.32833\n",
      "\tTotal predictions: 13000\tTrue positives:  661\tFalse positives: 1405\tFalse negatives: 1339\tTrue negatives: 9595\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=6\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81373\tPrecision: 0.29786\tRecall: 0.29250\tF1: 0.29516\tF2: 0.29356\n",
      "\tTotal predictions: 15000\tTrue positives:  585\tFalse positives: 1379\tFalse negatives: 1415\tTrue negatives: 11621\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=7\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.79827\tPrecision: 0.27057\tRecall: 0.30250\tF1: 0.28565\tF2: 0.29553\n",
      "\tTotal predictions: 15000\tTrue positives:  605\tFalse positives: 1631\tFalse negatives: 1395\tTrue negatives: 11369\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=8\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80233\tPrecision: 0.27251\tRecall: 0.28900\tF1: 0.28051\tF2: 0.28554\n",
      "\tTotal predictions: 15000\tTrue positives:  578\tFalse positives: 1543\tFalse negatives: 1422\tTrue negatives: 11457\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=9\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81280\tPrecision: 0.29367\tRecall: 0.28750\tF1: 0.29055\tF2: 0.28871\n",
      "\tTotal predictions: 15000\tTrue positives:  575\tFalse positives: 1383\tFalse negatives: 1425\tTrue negatives: 11617\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=10\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80367\tPrecision: 0.26003\tRecall: 0.25600\tF1: 0.25800\tF2: 0.25680\n",
      "\tTotal predictions: 15000\tTrue positives:  512\tFalse positives: 1457\tFalse negatives: 1488\tTrue negatives: 11543\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=11\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80333\tPrecision: 0.26508\tRecall: 0.26800\tF1: 0.26653\tF2: 0.26741\n",
      "\tTotal predictions: 15000\tTrue positives:  536\tFalse positives: 1486\tFalse negatives: 1464\tTrue negatives: 11514\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=12\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.82033\tPrecision: 0.32115\tRecall: 0.31200\tF1: 0.31651\tF2: 0.31379\n",
      "\tTotal predictions: 15000\tTrue positives:  624\tFalse positives: 1319\tFalse negatives: 1376\tTrue negatives: 11681\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=13\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.82453\tPrecision: 0.33542\tRecall: 0.32200\tF1: 0.32857\tF2: 0.32460\n",
      "\tTotal predictions: 15000\tTrue positives:  644\tFalse positives: 1276\tFalse negatives: 1356\tTrue negatives: 11724\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=14\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81540\tPrecision: 0.29688\tRecall: 0.28100\tF1: 0.28872\tF2: 0.28404\n",
      "\tTotal predictions: 15000\tTrue positives:  562\tFalse positives: 1331\tFalse negatives: 1438\tTrue negatives: 11669\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=15\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81773\tPrecision: 0.30725\tRecall: 0.29250\tF1: 0.29969\tF2: 0.29534\n",
      "\tTotal predictions: 15000\tTrue positives:  585\tFalse positives: 1319\tFalse negatives: 1415\tTrue negatives: 11681\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=16\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.82100\tPrecision: 0.32676\tRecall: 0.32300\tF1: 0.32487\tF2: 0.32374\n",
      "\tTotal predictions: 15000\tTrue positives:  646\tFalse positives: 1331\tFalse negatives: 1354\tTrue negatives: 11669\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=17\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81807\tPrecision: 0.31766\tRecall: 0.31750\tF1: 0.31758\tF2: 0.31753\n",
      "\tTotal predictions: 15000\tTrue positives:  635\tFalse positives: 1364\tFalse negatives: 1365\tTrue negatives: 11636\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=18\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81947\tPrecision: 0.31920\tRecall: 0.31250\tF1: 0.31582\tF2: 0.31382\n",
      "\tTotal predictions: 15000\tTrue positives:  625\tFalse positives: 1333\tFalse negatives: 1375\tTrue negatives: 11667\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=19\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81887\tPrecision: 0.31793\tRecall: 0.31300\tF1: 0.31544\tF2: 0.31397\n",
      "\tTotal predictions: 15000\tTrue positives:  626\tFalse positives: 1343\tFalse negatives: 1374\tTrue negatives: 11657\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=20\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'to_messages', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81673\tPrecision: 0.30805\tRecall: 0.30050\tF1: 0.30423\tF2: 0.30198\n",
      "\tTotal predictions: 15000\tTrue positives:  601\tFalse positives: 1350\tFalse negatives: 1399\tTrue negatives: 11650\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### testing results of DecisionTreeClassifier against all feature combinations using SelectKBest\n",
    "for i in range(2, (len(data_dict[data_dict.keys()[0]]))):\n",
    "    print '\\nSelectKBest k=' + str(i)\n",
    "    print 'used features: ' + str(get_kbest_features(df, i))\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    print test_classifier(clf, my_dataset, get_kbest_features(df, i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best F1 results of DecisionTreeClasifier Using Default Parameters\n",
    "\n",
    "        SelectKBest k=4\n",
    "        used features: ['poi', 'bonus', 'exercised_stock_options', 'total_stock_value']\n",
    "        DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                    max_features=None, max_leaf_nodes=None,\n",
    "                    min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                    presort=False, random_state=None, splitter='best')\n",
    "\n",
    "        Accuracy: 0.80238\n",
    "        Precision: 0.36485\n",
    "        Recall: 0.38400\n",
    "        F1: 0.37418\n",
    "        F2: 0.38001\n",
    "\n",
    "        Total predictions: 13000\n",
    "        True positives:  768\n",
    "        False positives: 1337\n",
    "        False negatives: 1232\n",
    "        True negatives: 9663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SelectKBest k=2\n",
      "used features: ['poi', 'exercised_stock_options']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85700\tPrecision: 0.25985\tRecall: 0.31000\tF1: 0.28272\tF2: 0.29848\n",
      "\tTotal predictions: 11000\tTrue positives:  310\tFalse positives:  883\tFalse negatives:  690\tTrue negatives: 9117\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=3\n",
      "used features: ['poi', 'exercised_stock_options', 'total_stock_value']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.81915\tPrecision: 0.37419\tRecall: 0.26100\tF1: 0.30751\tF2: 0.27781\n",
      "\tTotal predictions: 13000\tTrue positives:  522\tFalse positives:  873\tFalse negatives: 1478\tTrue negatives: 10127\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=4\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'total_stock_value']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.80562\tPrecision: 0.34618\tRecall: 0.29650\tF1: 0.31942\tF2: 0.30526\n",
      "\tTotal predictions: 13000\tTrue positives:  593\tFalse positives: 1120\tFalse negatives: 1407\tTrue negatives: 9880\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=5\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.80269\tPrecision: 0.30092\tRecall: 0.21350\tF1: 0.24978\tF2: 0.22667\n",
      "\tTotal predictions: 13000\tTrue positives:  427\tFalse positives:  992\tFalse negatives: 1573\tTrue negatives: 10008\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=6\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83440\tPrecision: 0.33845\tRecall: 0.25350\tF1: 0.28988\tF2: 0.26690\n",
      "\tTotal predictions: 15000\tTrue positives:  507\tFalse positives:  991\tFalse negatives: 1493\tTrue negatives: 12009\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=7\n",
      "used features: ['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.82193\tPrecision: 0.29654\tRecall: 0.24450\tF1: 0.26802\tF2: 0.25339\n",
      "\tTotal predictions: 15000\tTrue positives:  489\tFalse positives: 1160\tFalse negatives: 1511\tTrue negatives: 11840\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=8\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83747\tPrecision: 0.35243\tRecall: 0.26150\tF1: 0.30023\tF2: 0.27573\n",
      "\tTotal predictions: 15000\tTrue positives:  523\tFalse positives:  961\tFalse negatives: 1477\tTrue negatives: 12039\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=9\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83587\tPrecision: 0.34763\tRecall: 0.26350\tF1: 0.29977\tF2: 0.27690\n",
      "\tTotal predictions: 15000\tTrue positives:  527\tFalse positives:  989\tFalse negatives: 1473\tTrue negatives: 12011\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=10\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.81800\tPrecision: 0.27744\tRecall: 0.22750\tF1: 0.25000\tF2: 0.23600\n",
      "\tTotal predictions: 15000\tTrue positives:  455\tFalse positives: 1185\tFalse negatives: 1545\tTrue negatives: 11815\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=11\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.81573\tPrecision: 0.26764\tRecall: 0.22000\tF1: 0.24149\tF2: 0.22812\n",
      "\tTotal predictions: 15000\tTrue positives:  440\tFalse positives: 1204\tFalse negatives: 1560\tTrue negatives: 11796\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=12\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.82627\tPrecision: 0.31569\tRecall: 0.25950\tF1: 0.28485\tF2: 0.26908\n",
      "\tTotal predictions: 15000\tTrue positives:  519\tFalse positives: 1125\tFalse negatives: 1481\tTrue negatives: 11875\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=13\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.82680\tPrecision: 0.31679\tRecall: 0.25850\tF1: 0.28469\tF2: 0.26838\n",
      "\tTotal predictions: 15000\tTrue positives:  517\tFalse positives: 1115\tFalse negatives: 1483\tTrue negatives: 11885\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=14\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83407\tPrecision: 0.34516\tRecall: 0.27250\tF1: 0.30455\tF2: 0.28448\n",
      "\tTotal predictions: 15000\tTrue positives:  545\tFalse positives: 1034\tFalse negatives: 1455\tTrue negatives: 11966\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=15\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83427\tPrecision: 0.34717\tRecall: 0.27600\tF1: 0.30752\tF2: 0.28780\n",
      "\tTotal predictions: 15000\tTrue positives:  552\tFalse positives: 1038\tFalse negatives: 1448\tTrue negatives: 11962\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=16\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83920\tPrecision: 0.37205\tRecall: 0.29950\tF1: 0.33186\tF2: 0.31165\n",
      "\tTotal predictions: 15000\tTrue positives:  599\tFalse positives: 1011\tFalse negatives: 1401\tTrue negatives: 11989\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=17\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83880\tPrecision: 0.36888\tRecall: 0.29400\tF1: 0.32721\tF2: 0.30644\n",
      "\tTotal predictions: 15000\tTrue positives:  588\tFalse positives: 1006\tFalse negatives: 1412\tTrue negatives: 11994\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=18\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84013\tPrecision: 0.36821\tRecall: 0.27800\tF1: 0.31681\tF2: 0.29232\n",
      "\tTotal predictions: 15000\tTrue positives:  556\tFalse positives:  954\tFalse negatives: 1444\tTrue negatives: 12046\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=19\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84027\tPrecision: 0.36870\tRecall: 0.27800\tF1: 0.31699\tF2: 0.29239\n",
      "\tTotal predictions: 15000\tTrue positives:  556\tFalse positives:  952\tFalse negatives: 1444\tTrue negatives: 12048\n",
      "\n",
      "None\n",
      "\n",
      "SelectKBest k=20\n",
      "used features: ['poi', 'bonus', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'salary', 'shared_receipt_with_poi', 'to_messages', 'total_payments', 'total_stock_value', 'total_pay', 'from_poi_ratio', 'to_poi_ratio']\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83753\tPrecision: 0.35967\tRecall: 0.28000\tF1: 0.31487\tF2: 0.29298\n",
      "\tTotal predictions: 15000\tTrue positives:  560\tFalse positives:  997\tFalse negatives: 1440\tTrue negatives: 12003\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### testing results of AdaBoostClassifier against all feature combinations using SelectKBest\n",
    "for i in range(2, (len(data_dict[data_dict.keys()[0]]))):\n",
    "    print '\\nSelectKBest k=' + str(i)\n",
    "    print 'used features: ' + str(get_kbest_features(df, i))\n",
    "    clf = AdaBoostClassifier()\n",
    "    print test_classifier(clf, my_dataset, get_kbest_features(df, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best F1 results of AdaBoostClassifier Using Default Parameters\n",
    "\n",
    "        SelectKBest k=4\n",
    "        used features: ['poi', 'bonus', 'exercised_stock_options', 'total_stock_value']\n",
    "        AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=50, random_state=None)\n",
    "\n",
    "        Accuracy: 0.80515\n",
    "        Precision: 0.34461\n",
    "        Recall: 0.29550\n",
    "        F1: 0.31817\n",
    "        F2: 0.30417\n",
    "\n",
    "        Total predictions: 13000\n",
    "        True positives:  591\n",
    "        False positives: 1124\n",
    "        False negatives: 1409\n",
    "        True negatives: 9876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item: Tuning the Algorithm -\n",
    "\n",
    "##### Question 4 - What does it mean to tune the parameters of an algorithm, and what can happen if you dont do this well? How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).\n",
    "\n",
    "Tuning the parameters of an algorithm is the process of optimizing the parameters that impact the algorithm's behavior to achieve the best performance. The most basic way of tuning an algorithm is by trying multiple variations of the values the algorithm accepts and reviewing the results (For example, varying the value of k when using the SelectKBest algorhythm controlled the number of returned features). As you saw above, the number of returned features has a significant impact on our Classifier testing and we achieved better performance when we used different k values when testing the various classifiers. Not tuning the algorithms properly can have a significant and cascading effect on our results and the performance we can achieve from our classifiers. For example, choosing to use k = 2 in our feature selection because it had the highest accuracy rate when using the GassiuanNB classifier wouldn't have been the correct choice since in this instance accuracy is less important than the recall, precision and/or F1 values. \n",
    "\n",
    "My initial testing showed that I was able to get the best F1 score using the Gaussian NB with a list of 8 features generated by SelectKBest. Unfortunately, the Gaussian NB classifier has no tunable parameters so I performed my tuning on the Decision Tree and ADA Boost Classifiers instead. I tuned the parameters of those two classifiers by using GridSearchCV with an array that contained a list of possibilities for each parameter. GridSearchCV than ran through each possible combination and recommended the best parameter values. The GridSearchCV algorithm itself can also be tuned by specifying the kind of scoring and number of folds it should use when testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "\n",
    "data = featureFormat(my_dataset, get_kbest_features(df, 4), sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1250 candidates, totalling 12500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  50 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=5)]: Done 1210 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=5)]: Done 3210 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=5)]: Done 6010 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=5)]: Done 9610 tasks      | elapsed:   33.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'max_leaf_nodes': None, 'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 12500 out of 12500 | elapsed:   43.6s finished\n"
     ]
    }
   ],
   "source": [
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "               'min_samples_split': [2, 5, 8, 10, 20],\n",
    "               'max_depth': [None, 2, 5, 8, 10],\n",
    "               'min_samples_leaf': [1, 2, 5, 8, 10],\n",
    "               'max_leaf_nodes': [None, 2, 5, 10, 20]}\n",
    "svr = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(svr, parameters, scoring='f1', verbose=1, error_score='raise', n_jobs=5, cv = 10)\n",
    "clf.fit(features, labels)\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Tuned to Best F1 score\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.79900\tPrecision: 0.35882\tRecall: 0.38950\tF1: 0.37353\tF2: 0.38295\n",
      "\tTotal predictions: 13000\tTrue positives:  779\tFalse positives: 1392\tFalse negatives: 1221\tTrue negatives: 9608\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_f1_dtc = tree.DecisionTreeClassifier(min_samples_split=2, \n",
    "                                       max_leaf_nodes=None, \n",
    "                                       criterion=\"entropy\", \n",
    "                                       max_depth=10, \n",
    "                                       min_samples_leaf=1)\n",
    "print \"Decision Tree Classifier Tuned to Best F1 score\"\n",
    "print test_classifier(best_f1_dtc, my_dataset, get_kbest_features(df, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Untuned\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80138\tPrecision: 0.36364\tRecall: 0.38800\tF1: 0.37542\tF2: 0.38287\n",
      "\tTotal predictions: 13000\tTrue positives:  776\tFalse positives: 1358\tFalse negatives: 1224\tTrue negatives: 9642\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "default_dtc = tree.DecisionTreeClassifier()\n",
    "print \"Decision Tree Classifier Untuned\"\n",
    "print test_classifier(default_dtc, my_dataset, get_kbest_features(df, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 40, 'learning_rate': 1.2, 'algorithm': 'SAMME'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 120 out of 120 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_estimators': [30, 40, 50],\n",
    "               'algorithm': ['SAMME', 'SAMME.R'],\n",
    "               'learning_rate': [.5, .8, 1, 1.2]}\n",
    "svr = AdaBoostClassifier()\n",
    "clf = GridSearchCV(svr, parameters, scoring='f1', verbose=1, error_score='raise', n_jobs=5, cv = 5)\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA Boost Classifier Tuned to Best F1 score\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.2, n_estimators=40, random_state=None)\n",
      "\tAccuracy: 0.81138\tPrecision: 0.36595\tRecall: 0.30850\tF1: 0.33478\tF2: 0.31850\n",
      "\tTotal predictions: 13000\tTrue positives:  617\tFalse positives: 1069\tFalse negatives: 1383\tTrue negatives: 9931\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_f1_ada = AdaBoostClassifier(n_estimators=40,\n",
    "                                     learning_rate=1.2,\n",
    "                                     algorithm='SAMME.R')\n",
    "print \"ADA Boost Classifier Tuned to Best F1 score\"\n",
    "print test_classifier(best_f1_ada, my_dataset, get_kbest_features(df, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA Boost Classifier Untuned\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.80538\tPrecision: 0.34593\tRecall: 0.29750\tF1: 0.31989\tF2: 0.30607\n",
      "\tTotal predictions: 13000\tTrue positives:  595\tFalse positives: 1125\tFalse negatives: 1405\tTrue negatives: 9875\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "default_ada = AdaBoostClassifier()\n",
    "print \"ADA Boost Classifier Untuned\"\n",
    "print test_classifier(default_ada, my_dataset, get_kbest_features(df, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Item(s): Validation Strategy / Usage of Evaluation Metrics -\n",
    "\n",
    "##### Question 5: What is validation, and whats a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "One way to validate an algorithm is to divide the entire dataset into a learning dataset and a testing dataset. The learning dataset is used when training/fitting the classifiers. We then take the already fitted/trained classifier and use the testing dataset to evaluate the performance of our algorithm. A classic mistake that can be made here is using the same dataset to both train/fit and test. Since the data used for fitting includes the same values as the data used for testing, the data points presented to the algorithm during testing are not new and we are basically evaluating how the algorithm does against data it is familiar with, which doesn't tell us anything about how the algorithm would perform against unknown data. I performed my validation by excluding 30% of the dataset from the training set and used only the remaining 70% when training/fitting. I then used the excluded 30% to test the fitted classifier so I can evaluate how the algorithm performs against a previously unseen dataset.\n",
    "\n",
    "\n",
    "##### Question 6: Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human understandable about your algorithms performance. \n",
    "\n",
    "As I mentioned previously, my initial testing showed that I was able to get the best F1 score using the Gaussian NB using a list of 8 features generated by SelectKBest (poi, bonus, deferred_income, exercised_stock_options, salary, total_stock_value, total_pay, to_poi_ratio), but since the Gaussian NB classifer has no tunable parameters, I performed my tuning on the Decision Tree and ADA Boost Classifiers instead. \n",
    "\n",
    "One of the goals of this project was to be able to achieve precision and recall scores better than 0.3. Precision and Recall are defined as follows:\n",
    "\n",
    "- **Precision:** In our case, precision is defined as the probability that the prediction made by the algorithm is correct. In other words, if our algorithm indicates that a person is likely to be a POI based on the features it evaluates, the Precision is the fraction of individuals that should truly be classified as POIs from all the individuals identified as potential POIs using our algorithm.\n",
    "- **Recall:** Also known as sensitivity, is a measure of how many potential POIs our algorithm missed. In other words, the fraction of individuals that should have been classified as POIs based on the features evaluated by our algorithm that were evalauted as non-POIs.\n",
    "\n",
    "I was able to achieve precision and recall scores better than 0.3 in all cases, although the results of these two classifiers were lower than what I was able to achieve using the Gaussian NB classifier.\n",
    "\n",
    "The best scores I achieved for each classifier were as follows:\n",
    "    \n",
    "##### Gaussian NB Classifier - Default:\n",
    "        SelectKBest k=8\n",
    "        Features: poi, bonus, deferred_income, exercised_stock_options, salary, total_stock_value, total_pay, to_poi_ratio\n",
    "        \n",
    "        Accuracy: 0.86513\tPrecision: 0.49235\tRecall: 0.37000\t\n",
    "        F1: 0.42250\tF2: 0.38935\n",
    "        \n",
    "        Total predictions: 15000\t\n",
    "        True positives:  740\tFalse positives:  763\tFalse negatives: 1260\tTrue negatives: 12237\n",
    "        \n",
    "##### Decision Tree Classifier - Default:\n",
    "        SelectKBest k=4\n",
    "        Features: poi, bonus, exercised_stock_options, total_stock_value\n",
    "\n",
    "        Accuracy: 0.80138\tPrecision: 0.36364\tRecall: 0.38800\n",
    "        F1: 0.37542\tF2: 0.38287\n",
    "        \n",
    "        Total predictions: 13000\n",
    "        True positives:  776\tFalse positives: 1358\tFalse negatives: 1224\tTrue negatives: 9642\n",
    "\n",
    "##### ADA Booster Classifier - Tuned: algorithm='SAMME.R'; learning_rate=1.2; n_estimators=40\n",
    "        SelectKBest k=4\n",
    "        Features: poi, bonus, exercised_stock_options, total_stock_value\n",
    "\n",
    "        Accuracy: 0.81138\tPrecision: 0.36595\tRecall: 0.30850\n",
    "        F1: 0.33478\tF2: 0.31850\n",
    "        \n",
    "        Total predictions: 13000\n",
    "        True positives:  617\tFalse positives: 1069\tFalse negatives: 1383\tTrue negatives: 9931\n",
    "\n",
    "\n",
    "Overall, I feel that the Gaussian NB classifier performed best not only because of the higher scores in accuracy, precision, recall and F1/F2, but also because the Total prediction the tester function ran was 15K instead of 13K because of the larger number of features. In addition, the Gaussian NB classifier was the only classifier that performed better with a list of features that included any reference to information derived from e-mail data. The other two classifiers (Decision Tree and ADA Boost) performed best with only 4 features, none of which referenced any e-mail related information. These Classifiers performed best when the features they evaluated were not related to the e-mail data, which makes their result not very relevant to this project.\n",
    "\n",
    "That being said, using the Gaussian NB classifier introduces a fairly significant bias because of its \"Naive\" tendency to assume there is no relation between the features, which in our case is incorrect. Several of our features are a summation of other features (for example, the feature **total_pay** is a sum of **bonus, deferred_income, exercised_stock_options, salary, total_stock_value,** and several other features and clearly has a strong relationship to them).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = get_kbest_features(df, 8)\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Rubric Item: poi_id.py\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import random\n",
    "        import sys\n",
    "        import pickle\n",
    "        sys.path.append(\"../tools/\")\n",
    "\n",
    "        from feature_format import featureFormat, targetFeatureSplit\n",
    "        from tester import dump_classifier_and_data\n",
    "\n",
    "        #from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "        from sklearn.feature_selection import SelectKBest, f_regression\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "        from sklearn import tree\n",
    "        from sklearn.grid_search import GridSearchCV\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        from tester import test_classifier\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        from sklearn import svm\n",
    "\n",
    "        def get_kbest_features(df, k):\n",
    "            sel_kbest = SelectKBest(f_regression, k)\n",
    "            sel_kbest.fit(df, df['poi'])\n",
    "            features_SelectKBest = df.columns[sel_kbest.get_support()].tolist()\n",
    "            return features_SelectKBest\n",
    "\n",
    "        ### Task 1: Select what features you'll use.\n",
    "        ### features_list is a list of strings, each of which is a feature name.\n",
    "        ### The first feature must be \"poi\".\n",
    "        ### features_list = ['poi','salary'] # You will need to use more features\n",
    "\n",
    "        features_list = ['poi',\n",
    "                         'bonus',\n",
    "                         'deferral_payments',\n",
    "                         'deferred_income',\n",
    "                         'director_fees',\n",
    "                         # 'email_address', #excluding non-numeric value\n",
    "                         'exercised_stock_options',\n",
    "                         'expenses',\n",
    "                         'from_messages',\n",
    "                         'from_poi_to_this_person',\n",
    "                         'from_this_person_to_poi',\n",
    "                         'loan_advances',\n",
    "                         'long_term_incentive',\n",
    "                         'other',\n",
    "                         'restricted_stock',\n",
    "                         'restricted_stock_deferred',\n",
    "                         'salary',\n",
    "                         'shared_receipt_with_poi',\n",
    "                         'to_messages',\n",
    "                         'total_payments',\n",
    "                         'total_stock_value'\n",
    "                        ]\n",
    "\n",
    "        ### Load the dictionary containing the dataset\n",
    "        with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "            data_dict = pickle.load(data_file)\n",
    "\n",
    "\n",
    "        ### Task 2: Remove outliers\n",
    "        data_dict.pop('TOTAL', 0)\n",
    "\n",
    "\n",
    "        ### Task 3: Create new feature(s)\n",
    "        ### Store to my_dataset for easy export below.\n",
    "\n",
    "        data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "        df = pd.DataFrame(data, columns = ['poi',\n",
    "                         'bonus',\n",
    "                         'deferral_payments',\n",
    "                         'deferred_income',\n",
    "                         'director_fees',\n",
    "                         #'email_address', #excluding non-numeric value\n",
    "                         'exercised_stock_options',\n",
    "                         'expenses',\n",
    "                         'from_messages',\n",
    "                         'from_poi_to_this_person',\n",
    "                         'from_this_person_to_poi',\n",
    "                         'loan_advances',\n",
    "                         'long_term_incentive',\n",
    "                         'other',\n",
    "                         'restricted_stock',\n",
    "                         'restricted_stock_deferred',\n",
    "                         'salary',\n",
    "                         'shared_receipt_with_poi',\n",
    "                         'to_messages',\n",
    "                         'total_payments',\n",
    "                         'total_stock_value'\n",
    "                        ])\n",
    "\n",
    "        df['total_pay'] = df['total_stock_value'] + df['total_payments']\n",
    "        df['salary_total_pay_ratio'] = df['salary'] / df['total_pay']\n",
    "\n",
    "        df['from_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "        df['to_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "\n",
    "        # replacing all NaN with 0\n",
    "        df = df.replace({np.nan: 0})\n",
    "\n",
    "        ### Store to my_dataset for easy export below.\n",
    "        my_dataset = df.to_dict(orient='index')\n",
    "\n",
    "\n",
    "        ### Extract features and labels from dataset for local testing\n",
    "\n",
    "        features_list = get_kbest_features(df, 8)\n",
    "\n",
    "        data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "        labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "        ### Task 4: Try a varity of classifiers\n",
    "        ### Please name your classifier clf for easy export below.\n",
    "        ### Note that if you want to do PCA or other multi-stage operations,\n",
    "        ### you'll need to use Pipelines. For more info:\n",
    "        ### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "        # Provided to give you a starting point. Try a variety of classifiers.\n",
    "        # from sklearn.naive_bayes import GaussianNB\n",
    "        clf = GaussianNB()\n",
    "\n",
    "        ### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "        ### using our testing script. Check the tester.py script in the final project\n",
    "        ### folder for details on the evaluation method, especially the test_classifier\n",
    "        ### function. Because of the small size of the dataset, the script uses\n",
    "        ### stratified shuffle split cross validation. For more info: \n",
    "        ### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "        # Example starting point. Try investigating other evaluation techniques!\n",
    "        # from sklearn.cross_validation import train_test_split\n",
    "        features_train, features_test, labels_train, labels_test = \\\n",
    "            train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "        clf.fit(features_train, labels_train)\n",
    "\n",
    "        ### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "        ### check your results. You do not need to change anything below, but make sure\n",
    "        ### that the version of poi_id.py that you submit can be run on its own and\n",
    "        ### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "        dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
